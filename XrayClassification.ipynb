{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-vR5oWgY5rk"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam, Adamax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.applications import MobileNetV2, InceptionV3, DenseNet121\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import (\n",
        "Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, GlobalAveragePooling2D\n",
        ")\n",
        "from tensorflow.keras import mixed_precision, regularizers\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from imutils import paths\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import shutil\n",
        "import cv2\n",
        "import os\n",
        "import multiprocessing\n",
        "mixed_precision.set_global_policy('mixed_float16')\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download dataset"
      ],
      "metadata": {
        "id": "AalGcMnaZD2P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = kagglehub.dataset_download(\"tawsifurrahman/covid19-radiography-database\")\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "id": "cO0Cgo6wZARr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_dir =\n",
        "\"/kaggle/input/covid19-radiography-database/COVID-\n",
        "19_Radiography_Dataset\"\n",
        "image_folders = {\n",
        "\"COVID\": os.path.join(dataset_dir,\"COVID/images\"),\n",
        "\"Lung_Opacity\": os.path.join(dataset_dir,\"Lung_Opacity/images\"),\n",
        "\"Normal\": os.path.join(dataset_dir,\"Normal/images\"),\n",
        "\"Viral Pneumonia\": os.path.join(dataset_dir,\"Viral Pneumonia/images\"),\n",
        "}\n",
        "output_dir =\"processed_dataset\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "def copy_images(category,src_path):\n",
        "  dest_path = os.path.join(output_dir, category)\n",
        "  os.makedirs(dest_path, exist_ok=True)\n",
        "  for img in os.listdir(src_path):\n",
        "    shutil.copy(os.path.join(src_path, img), os.path.join(dest_path, img))\n",
        "\n",
        "for category, path in image_folders.items():\n",
        "  os.makedirs(f\"processed_dataset/{category}\", exist_ok=True)\n",
        "  for img in os.listdir(path):\n",
        "    shutil.copy(os.path.join(path, img),f\"processed_dataset/{category}/{img}\")\n",
        "with multiprocessing.Pool(processes=4) as pool:\n",
        "  pool.starmap(copy_images, image_folders.items())"
      ],
      "metadata": {
        "id": "H9teh_pcZPJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Augmentation and Data slpit**"
      ],
      "metadata": {
        "id": "H1Fkf6YmaSsa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_size = (224,224)\n",
        "batch_size =32\n",
        "train_datagen = ImageDataGenerator(\n",
        "rescale=1./255,\n",
        "rotation_range=20,\n",
        "width_shift_range=0.2,\n",
        "height_shift_range=0.2,\n",
        "shear_range=0.2,\n",
        "zoom_range=0.2,\n",
        "horizontal_flip=True,\n",
        "validation_split=0.2\n",
        ")\n",
        "val_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "\"processed_dataset\",\n",
        "target_size=img_size,\n",
        "batch_size=batch_size,\n",
        "class_mode=\"categorical\",\n",
        "subset=\"training\",\n",
        "shuffle=True\n",
        ")\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "\"processed_dataset\",\n",
        "target_size=img_size,\n",
        "batch_size=batch_size,\n",
        "class_mode='categorical',\n",
        "subset=\"validation\",\n",
        "shuffle=True\n",
        ")\n",
        ""
      ],
      "metadata": {
        "id": "kk6dNwkzaYYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(base_model_class, input_shape=(224,224,3),num_classes=4,dropout_rate=0.5,regularization_rate=0.02):\n",
        "  base_model = base_model_class(input_shape=input_shape, include_top=False, weights='imagenet', pooling='max')\n",
        "  base_model.trainable =True\n",
        "  for layer in base_model.layers[:-20]:\n",
        "    layer.trainable =False\n",
        "  model = Sequential([base_model,\n",
        "  BatchNormalization(),\n",
        "  Dense(256, activation='relu', kernel_regularizer=regularizers.l2(regularization_rate)),\n",
        "  Dropout(dropout_rate),\n",
        "  Dense(num_classes, activation='softmax')\n",
        "  ])\n",
        "  return model"
      ],
      "metadata": {
        "id": "6vaJfGJcbLYZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model,train_generator,val_generator,learning_rate=0.001,epochs=15,class_weights=None,patience=3):\n",
        "  model.compile(optimizer=Adamax(learning_rate=learning_rate),loss='categorical_crossentropy',metrics=['accuracy','recall'])\n",
        "  early_stopping = EarlyStopping(monitor='val_loss',# stops when validation loss stops improving\n",
        "  patience=patience,# number of epochs to wait before stopping\n",
        "  restore_best_weights=True)\n",
        "  history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=epochs,\n",
        "    verbose=1,\n",
        "    class_weight=class_weights,\n",
        "    callbacks=[early_stopping])\n",
        "  return model, history\n",
        "\n",
        "\n",
        "\n",
        "def evaluate_model(model,val_generator):\n",
        "  y_true = np.array([val_generator.classes[i] for i in val_generator.index_array])\n",
        "  y_pred_probs = model.predict(val_generator)\n",
        "  y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "  cm = confusion_matrix(y_true, y_pred)\n",
        "  plt.figure(figsize=(8,6))\n",
        "  sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=val_generator.class_indices.keys(),yticklabels=val_generator.class_indices.keys())\n",
        "  plt.xlabel(\"Predicted Label\")\n",
        "  plt.ylabel(\"True Label\")\n",
        "  plt.title(\"Confusion Matrix\")\n",
        "  plt.show()\n",
        "  print(classification_report(y_true, y_pred, target_names=val_generator.class_indices.keys()))\n",
        "  return y_pred_probs\n",
        "\n",
        "def plot_metrics(history,y_pred_probs,val_generator,num_classes=4):\n",
        "  epochs_range =range(1,len(history.history['accuracy']) +1)\n",
        "  plt.figure(figsize=(8,5))\n",
        "  plt.plot(epochs_range, history.history['accuracy'], label='Training Accuracy', linestyle='-', marker='o')\n",
        "  plt.plot(epochs_range, history.history['val_accuracy'], label='Validation Accuracy', linestyle='--', marker='o')\n",
        "  plt.plot(epochs_range, history.history['recall'], label='Training Recall', linestyle='-', marker='s')\n",
        "  plt.plot(epochs_range, history.history['val_recall'], label='Validation Recall', linestyle='--', marker='s')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Metric Value')\n",
        "  plt.legend()\n",
        "  plt.title('Accuracy & Recall')\n",
        "  plt.show()\n",
        "  plt.figure(figsize=(8,5))\n",
        "  plt.plot(epochs_range, history.history['loss'], label= 'Training Loss', linestyle='-', marker='o')\n",
        "  plt.plot(epochs_range, history.history[ 'val_loss'], label='Validation Loss', linestyle='--', marker='o')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend()\n",
        "  plt.title('Model Loss')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "27OAC3sqbvh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CHEXNET MODEL**"
      ],
      "metadata": {
        "id": "i9ORjwC7eYMa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "input_shape = (224,224,3)\n",
        "num_classes =4\n",
        "learning_rate =0.001\n",
        "epochs =10\n",
        "dropout_rate =0.5\n",
        "regularization_rate =0\n",
        "# Build model\n",
        "model = build_model(DenseNet121, input_shape, num_classes, dropout_rate, regularization_rate)\n",
        "# Train model\n",
        "model, history = train_model(model, train_generator, val_generator, learning_rate, epochs)\n",
        "# Evaluate model\n",
        "y_pred_probs = evaluate_model(model, val_generator)\n",
        "# Plot results\n",
        "plot_metrics(history, y_pred_probs, val_generator,num_classes)"
      ],
      "metadata": {
        "id": "KkYIau8FecEw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}